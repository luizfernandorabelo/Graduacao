{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ee997304",
      "metadata": {},
      "source": [
        "# Avaliação de Redes Complexas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c923da55",
      "metadata": {},
      "source": [
        "**Nome:** Luiz Fernando Rabelo (11796893)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "96f232ae",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd9433c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import EoN\n",
        "import pandas as pd\n",
        "from community import community_louvain\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6d0c71c2",
      "metadata": {},
      "source": [
        "**1 -** (a) Crie o gráfo a seguir usando a biblioteca networkx e mostre o gráfo.<br>\n",
        "Link para visualizar o grafo: https://commons.wikimedia.org/wiki/File:6n-graf.png<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de715272",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criação da lista de vértices:\n",
        "vertices = [1, 2, 3, 4, 5, 6]\n",
        "\n",
        "# Criação da lista de arestas:\n",
        "arestas = [(1,2), (1,5), (2,3), (2,5), (3,4), (4,5), (4,6)]\n",
        "\n",
        "# Geração do grafo a partir dos vértices e arestas criados:\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(vertices)\n",
        "G.add_edges_from(arestas)\n",
        "\n",
        "# Desenho do grafo:\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos=pos, with_labels=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e1943c2c",
      "metadata": {},
      "source": [
        "(b) Simule uma caminha aleatória nesse grafo e monte uma matriz onde o elemento Mij representa o número de visitas ao vértice j dado que a caminhada iniciou em i. Considere pelo menos 100 passos. Compare o número de visitas com a medida eigenvector centrality de cada vértice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb29a9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicialização de variáveis:\n",
        "n = len(G)\n",
        "total_passos = 3000\n",
        "matriz_visitas = np.zeros((n, n))\n",
        "matriz_adjacencia = nx.to_numpy_array(G)\n",
        "matriz_transicao = matriz_adjacencia / matriz_adjacencia.sum(axis=1)[:,None]\n",
        "\n",
        "# Caminhada aleatória:\n",
        "for vertice_inicial in range(1, n+1):\n",
        "    vertice_atual = vertice_inicial\n",
        "    for contagem_passos in range(total_passos):\n",
        "        matriz_visitas[vertice_inicial-1][vertice_atual-1] += 1\n",
        "        vertice_atual = np.random.choice(G.nodes, p=matriz_transicao[vertice_atual-1])\n",
        "\n",
        "# Cálculo da eigenvector centrality:\n",
        "eigen_centrality = np.array(list(nx.eigenvector_centrality(G, max_iter=total_passos).values()))\n",
        "\n",
        "# Normalização da matriz de visitas e do eigenvector:\n",
        "matriz_visitas /= total_passos\n",
        "eigen_centrality /= eigen_centrality.sum()\n",
        "\n",
        "# Cálculo da média de visitas na matriz computada na caminhada aleatória:\n",
        "media_visitas = [np.mean(matriz_visitas[:,i]) for i in range(n)]\n",
        "\n",
        "# Desenho gráfico dos resultados:\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Comparação Caminhadas Aleatórias vs Medida Eigenvector Centrality')\n",
        "plt.xlabel('Vértices')\n",
        "plt.ylabel('% Visitas')\n",
        "plt.plot(G.nodes, eigen_centrality, 'o-', ms=9, label='Eigenvector Centrality')\n",
        "for i in range(n):\n",
        "    plt.plot(G.nodes, matriz_visitas[i], 'o', label=f'Caminhada Aleatória início={i+1}')\n",
        "plt.plot(G.nodes, media_visitas, 'o-', ms=9, label='Média Caminhadas Aleatórias')\n",
        "plt.legend(loc=(1.01, 0));"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "acf1e5bd",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "40f23d2e",
      "metadata": {},
      "source": [
        "**2 -** (a) Leia a rede do arquivo advogato.txt e mostre a distribuição acumulada complementar do grau, isto é, $P(K > k)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95d9534",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leitura da rede advogato:\n",
        "G = nx.read_edgelist('data/advogato.txt')\n",
        "\n",
        "# Seleção do maior componente conectado:\n",
        "G = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
        "\n",
        "# Transformação dos labels de strings para inteiros:\n",
        "G = nx.convert_node_labels_to_integers(G)\n",
        "\n",
        "# Determinação da distribuição do grau:\n",
        "graus = np.array(list(dict(G.degree()).values()))\n",
        "grau_maximo = np.max(graus)\n",
        "ocorrencias = np.zeros(grau_maximo + 1)\n",
        "for grau in graus:\n",
        "    ocorrencias[grau] += 1\n",
        "\n",
        "# Determinação da distribuição acumulada complementar do grau:\n",
        "dist_acc_complementar = np.zeros(grau_maximo + 1)\n",
        "dist_acc_complementar[0] = len(graus) - ocorrencias[0]\n",
        "for i in range(1, grau_maximo + 1):\n",
        "    dist_acc_complementar[i] = len(graus) - ocorrencias[i] - dist_acc_complementar[i-1]\n",
        "\n",
        "# Representação do resultado:\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Distribuição Acumulada Complementar do Grau')\n",
        "fig = plt.subplot(1, 1, 1)\n",
        "fig.set_xscale('log')\n",
        "fig.set_yscale('log')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Pa(k)')\n",
        "plt.plot(ocorrencias, 'o', ms=5, mec='black');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "43648a8a",
      "metadata": {},
      "source": [
        "(b) Mostre o gráfico de Knn(k), ou seja, o grau médio dos vizinhos dos vértices de grau k em função do grau k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebfbbea6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determinação da lista de graus médios da vizinhança:\n",
        "graus_medios_vizinhanca = np.array([\n",
        "    float(nx.average_neighbor_degree(G, nodes=[vertice])[vertice]) for vertice in G.nodes\n",
        "])\n",
        "\n",
        "# Determinação da lista de graus de todos os vértices: \n",
        "graus_vertices = list(dict(G.degree()).values())\n",
        "\n",
        "# Inicialização das listas de vizinhos e de graus médios:\n",
        "graus_vizinhos = []\n",
        "clusterings_medios = []\n",
        "\n",
        "# Preenchimento da lista de vizinhos e de graus médios:\n",
        "for grau in np.arange(np.min(graus_vertices), np.max(graus_vertices) + 1):\n",
        "    grau_atual = graus_vertices == grau\n",
        "    if len(graus_medios_vizinhanca[grau_atual]) > 0:\n",
        "        graus_vizinhos.append(grau)\n",
        "        clusterings_medios.append(np.mean(graus_medios_vizinhanca[grau_atual]))\n",
        "\n",
        "# Desenho de knn(k) em função de k:\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "plt.title('Grau Médio dos Vizinhos dos Vértices de Grau k')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('knn(k)')\n",
        "plt.plot(graus_vizinhos, clusterings_medios, 'o', ms=5, mec='black', label='Grau médio dos vizinhos')\n",
        "\n",
        "# Desenho da melhor reta de aproximação dos pontos plotados:\n",
        "coeficientes = np.polyfit(graus_vizinhos, clusterings_medios, deg=1, full=True)\n",
        "a, b = coeficientes[0][0], coeficientes[0][1]\n",
        "xs = [min(graus_vizinhos), max(graus_vizinhos)]\n",
        "ys = [a*x + b for x in xs]\n",
        "plt.plot(xs, ys, '--', lw=2, label=f'Aproximação {round(a, 2)}x+{round(b,3)}');\n",
        "plt.legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "10cf8e13",
      "metadata": {},
      "source": [
        "(c) Calcule a matriz de menores distâncias $D$ a matriz $A^n$, que presenta o número de caminhadas de comprimento $n$ entre cada par de vértices. Faça um gráfico da correlação entre $D_{ij}$ e $A_{ij}$ para diferentes valores de $n$. Ou seja, um gráfico em que o eixo x representa n e o y, a correlação entre essas duas matrizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a54178e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicialização da matriz de distâncias:\n",
        "n = len(G)\n",
        "distancias = np.zeros((n,n))\n",
        "\n",
        "# Preenchimento da matriz de distâncias:\n",
        "for i in range(n):\n",
        "    for j in range(i+1, n):\n",
        "        if i != j:\n",
        "            distancias[i][j] = distancias[j][i] = len(nx.shortest_path(G, i, j)) - 1\n",
        "\n",
        "# Determinação da matriz A de adjacências:\n",
        "matriz_adjacencia = nx.to_numpy_array(G)\n",
        "\n",
        "# Deteminação do número de caminhadas aleatórias a partir da matriz A:\n",
        "comprimento_maximo = 10\n",
        "matrizes_caminhadas = [matriz_adjacencia]\n",
        "for _ in range(1, comprimento_maximo):\n",
        "    matrizes_caminhadas.append(matrizes_caminhadas[-1] @ matriz_adjacencia)\n",
        "\n",
        "# Cálculo das correlações de Pearson entre a distância e caminhadas:\n",
        "correlacoes = []\n",
        "for i in range(comprimento_maximo):\n",
        "    correlacoes.append(np.corrcoef(distancias.flatten(), matrizes_caminhadas[i].flatten())[0,1])\n",
        "\n",
        "# Desenho do resultado:\n",
        "xs = [i+1 for i in range(comprimento_maximo)]\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Correlação entre Menores Distâncias e Número de Caminhadas de Comprimento n')\n",
        "plt.xlabel('n')\n",
        "plt.ylabel('ρ(Dij, Aij^n)')\n",
        "plt.xticks(xs)\n",
        "plt.plot(xs, correlacoes, 'o-');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "699295a2",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "146d7e5f",
      "metadata": {},
      "source": [
        "**3 -** Considere a rede do arquivo internet_routers-22july06.gml. Escreva um código que calcule as seguintes medidas: (a) grau médio, (b) transitividade, (c) segundo momento da distribuição do grau, (d) entropia de Shannon da distribuição do grau, (e) coeficiente de assortatividade. Considere apenas o maior componente. Armazene os valoes em uma lista e imprima essa lista, indicando os valores de cada medida. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b1de14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leitura da rede internet routers:\n",
        "G = nx.read_gml('data/internet_routers-22july06.gml')\n",
        "\n",
        "# Seleção do maior componente conectado:\n",
        "G = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
        "\n",
        "# Transformação dos labels de strings para inteiros:\n",
        "G = nx.convert_node_labels_to_integers(G)\n",
        "\n",
        "# Cálculo do grau médio:\n",
        "grau_medio = np.mean(np.array(list(dict(G.degree()).values())))\n",
        "\n",
        "# Cálculo da transitividade:\n",
        "transitividade = nx.transitivity(G)\n",
        "\n",
        "# Cálculo do segundo momento da distribuição do grau:\n",
        "segundo_momento = 0\n",
        "for vertice in G.nodes:\n",
        "    segundo_momento += G.degree(vertice) ** 2\n",
        "segundo_momento /= len(G)\n",
        "\n",
        "# Cálculo da entropia de Shannon da distribuição do grau:\n",
        "graus = np.array(list(dict(G.degree()).values()))\n",
        "grau_maximo = np.max(graus)\n",
        "ocorrencias = np.zeros(grau_maximo + 1)\n",
        "for grau in graus:\n",
        "    ocorrencias[grau] += 1\n",
        "distribuicao_normalizada = ocorrencias / sum(ocorrencias)\n",
        "entropia = -sum([p * np.log2(p) for p in distribuicao_normalizada if p > 0])\n",
        "\n",
        "# Cálculo do coeficiente de assortatividade:\n",
        "assortatividade = nx.degree_assortativity_coefficient(G)\n",
        "\n",
        "# Mostragem da lista de resultados:\n",
        "print('- Grau médio =', grau_medio)\n",
        "print('- Transitividade =', transitividade)\n",
        "print('- Segundo momento do grau =', segundo_momento)\n",
        "print('- Entropia de Shannon do grau =', entropia)\n",
        "print('- Coeficiente de assortatividade =', assortatividade)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "517f2fba",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b3bbdeb",
      "metadata": {},
      "source": [
        "**4 -** Simule o modelo de propagação de epidemias SIR na rede de aeroportos dos EUA (USairport_2010.txt). Calcule a correlação de Pearson entre a fração final de recuperados e as seguintes medidas de centralidade: (a) grau, (b) betweenness centrality, (c) eigenvector centrality, (d) closenness centrality. Mostre os resultados e indique a maior correlação. Considere $\\beta = 0.06$ e $\\mu = 0.1$ no modelo SIR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e940ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leitura da rede us airport:\n",
        "G = nx.read_weighted_edgelist('data/usairport.txt')\n",
        "\n",
        "# Seleção do maior componente conectado:\n",
        "G = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
        "\n",
        "# Transformação dos labels de strings para inteiros:\n",
        "G = nx.convert_node_labels_to_integers(G)\n",
        "\n",
        "# Definição das taxas de propagação beta e mu:\n",
        "beta = 0.06   # contágio\n",
        "mu = 0.1      # recuperação\n",
        "\n",
        "# Definição do número de experimentos por vértice semente:\n",
        "total_experimentos = 5\n",
        "\n",
        "# Inicialização de variáveis complementares:\n",
        "fracoes_sucetiveis = []\n",
        "fracoes_infectados = []\n",
        "fracoes_recuperados = []\n",
        "recuperados_finais_por_vertice = []\n",
        "tempo_maximo = -1\n",
        "n = len(G)\n",
        "\n",
        "# Propagação da epidemia:\n",
        "for vertice in G.nodes:\n",
        "    recuperados_vertice = []\n",
        "    for _ in range(total_experimentos):\n",
        "        tempos, S, I, R = EoN.fast_SIR(G, beta, mu, initial_infecteds=vertice)\n",
        "        recuperados_vertice.append(R[-1])\n",
        "        tempo_maximo = max(tempo_maximo, len(tempos))\n",
        "        fracoes_sucetiveis.append(S / n)\n",
        "        fracoes_infectados.append(I / n)\n",
        "        fracoes_recuperados.append(R / n)\n",
        "    recuperados_finais_por_vertice.append(np.mean(recuperados_vertice))\n",
        "\n",
        "# Cálculo das médias percentuais de sucetíveis:\n",
        "medias_sucetiveis = np.zeros(tempo_maximo)\n",
        "for i in range(len(fracoes_sucetiveis)):\n",
        "    for j in range(len(fracoes_sucetiveis[i])):\n",
        "        medias_sucetiveis[j] = medias_sucetiveis[j] + fracoes_sucetiveis[i][j]\n",
        "medias_sucetiveis /= len(fracoes_sucetiveis)\n",
        "\n",
        "# Cálculo das médias percentuais de infectados:\n",
        "medias_infectados = np.zeros(tempo_maximo)\n",
        "for i in range(len(fracoes_infectados)):\n",
        "    for j in range(len(fracoes_infectados[i])):\n",
        "        medias_infectados[j] += fracoes_infectados[i][j]\n",
        "medias_infectados /= len(fracoes_infectados)\n",
        "\n",
        "# Cálculo das médias percentuais de recuperados:\n",
        "medias_recuperados = np.zeros(tempo_maximo)\n",
        "for i in range(len(fracoes_recuperados)):\n",
        "    for j in range(len(fracoes_recuperados[i])):\n",
        "        medias_recuperados[j] += fracoes_recuperados[i][j]\n",
        "medias_recuperados /= len(fracoes_recuperados)\n",
        "\n",
        "# Normalização do array de recuperados finais:\n",
        "recuperados_finais_por_vertice = np.array(recuperados_finais_por_vertice) / n\n",
        "print('Percentual médio de recuperados finais:', np.mean(recuperados_finais_por_vertice))\n",
        "\n",
        "# Desenho da evolução da epidemia:\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Evolução Modelo SIR (médias n sementes)')\n",
        "plt.xlabel('t')\n",
        "plt.ylabel('% vértices')\n",
        "plt.plot(range(tempo_maximo), medias_sucetiveis, '-o', label='Vértices Sucetíveis')\n",
        "plt.plot(range(tempo_maximo), medias_infectados, '-o', label='Vértices Infectados')\n",
        "plt.plot(range(tempo_maximo), medias_recuperados, '-o', label='Vértices Recuperados')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f11bc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determinação das medidas de centralidade:\n",
        "medidas_centralidade = [\n",
        "    list(dict(G.degree()).values()),                            # grau\n",
        "    list(dict(nx.betweenness_centrality(G)).values()),          # betweeness centrality\n",
        "    list(nx.eigenvector_centrality(G, max_iter=2000).values()), # eigenvector centrality\n",
        "    list(nx.closeness_centrality(G).values())                   # closeness centrality\n",
        "]\n",
        "\n",
        "# Determinação da lista dos rótulos de cada medida:\n",
        "labels_medidas = ['Grau', 'Betweeness Centrality', 'Eigenvector Centrality', 'Closeness Centrality']\n",
        "\n",
        "# Cálculo e mostragem das correlações:\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Correlações entre Recuperados Finais e Medidas de Centralidade')\n",
        "plt.xlabel('Medidas de Centralidade')\n",
        "plt.ylabel('Correlação com Recuperados Finais')\n",
        "plt.xticks([i for i in range(len(medidas_centralidade))], labels_medidas)\n",
        "for i in range(len(medidas_centralidade)):\n",
        "    correlacao = np.corrcoef(recuperados_finais_por_vertice, medidas_centralidade[i])[0,1]\n",
        "    print(f'- Correlação entre Fração de Recuperados Finais e {labels_medidas[i]} = {correlacao}')\n",
        "    plt.plot([i], [correlacao], 'P', ms='15')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "69dd4700",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0d2c1b80",
      "metadata": {},
      "source": [
        "**5 -** Gere redes do tipo BA, ER e WS (p=0.05) com grau médio igual a 10 e N = 500. Desenvolva um estudo para mostrar que essas apresentam topologias diferentes. (Seja criativ@)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0abfa38f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definição dos parâmetros das redes:\n",
        "n = 500\n",
        "grau_medio = 10\n",
        "p = 0.05\n",
        "total_experimentos = 50\n",
        "\n",
        "# Inicialização de dicionário de dados:\n",
        "dados_coletados = {\n",
        "    tipo_rede: {\n",
        "        'primeiro_momento': [],\n",
        "        'segundo_momento': [],\n",
        "        'clustering_medio': [],\n",
        "        'modularidade': [],\n",
        "        'media_menor_caminho': [],\n",
        "        'assortatividade_grau': [],\n",
        "    }\n",
        "    for tipo_rede in ['BA', 'ER', 'WS']\n",
        "}\n",
        "\n",
        "# Execução de experimentos:\n",
        "for _ in range(total_experimentos):\n",
        "\n",
        "    # Geração das redes:\n",
        "    redes = {\n",
        "        'BA': nx.barabasi_albert_graph(n, grau_medio),\n",
        "        'ER': nx.erdos_renyi_graph(n, p),\n",
        "        'WS': nx.watts_strogatz_graph(n, grau_medio, p)\n",
        "    }\n",
        "    \n",
        "    # Cálculo das features:\n",
        "    for tipo_rede in redes.keys():\n",
        "        G = redes[tipo_rede]\n",
        "        \n",
        "        # Determinação do primeiro momento do grau:\n",
        "        soma = 0\n",
        "        for node in G.nodes:\n",
        "            soma += G.degree(node)\n",
        "        dados_coletados[tipo_rede]['primeiro_momento'].append(soma / n)\n",
        "\n",
        "        # Determinação do segundo momento do grau:\n",
        "        soma = 0\n",
        "        for node in G.nodes:\n",
        "            soma += G.degree(node) ** 2\n",
        "        dados_coletados[tipo_rede]['segundo_momento'].append(soma / n)\n",
        "\n",
        "        # Determinação do average clustering:\n",
        "        clustering_medio = nx.average_clustering(G)\n",
        "        dados_coletados[tipo_rede]['clustering_medio'].append(clustering_medio)\n",
        "\n",
        "        # Determinação da modularidade:\n",
        "        particao = list(community_louvain.best_partition(G).values())\n",
        "        matriz_adjacencia = nx.adjacency_matrix(G)\n",
        "        total_arestas = G.number_of_edges()\n",
        "        soma = 0\n",
        "        for i in np.arange(n):\n",
        "            ki = len(list(G.neighbors(i)))\n",
        "            for j in np.arange(n):\n",
        "                if particao[i] == particao[j]:\n",
        "                    kj = len(list(G.neighbors(j)))\n",
        "                    soma += matriz_adjacencia[i,j] - (ki * kj) / (2 * total_arestas)\n",
        "        dados_coletados[tipo_rede]['modularidade'].append(soma / (2 * total_arestas))\n",
        "\n",
        "        # Determinação do menor caminho médio:\n",
        "        caminho_medio = nx.average_shortest_path_length(G)\n",
        "        dados_coletados[tipo_rede]['media_menor_caminho'].append(caminho_medio)\n",
        "\n",
        "        # Determinação do coeficiente de assortatividade do grau:\n",
        "        assortatividade = nx.degree_assortativity_coefficient(G)\n",
        "        dados_coletados[tipo_rede]['assortatividade_grau'].append(assortatividade)\n",
        "\n",
        "# Criação de dataframes para cada tipo de rede:\n",
        "df_ba = pd.DataFrame.from_dict(dados_coletados['BA'])\n",
        "print('Resultados Experimentos em Rede Barabasi Albert')\n",
        "display(df_ba)\n",
        "df_er = pd.DataFrame.from_dict(dados_coletados['ER'])\n",
        "print('Resultados Experimentos em Rede Erdos Renyi')\n",
        "display(df_er)\n",
        "df_ws = pd.DataFrame.from_dict(dados_coletados['WS'])\n",
        "print('Resultados Experimentos em Rede Watts Strogatz')\n",
        "display(df_ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9158458b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adição de labels para os tipos de rede:\n",
        "df_ba['tipo_rede'] = 'BA'\n",
        "df_er['tipo_rede'] = 'ER'\n",
        "df_ws['tipo_rede'] = 'WS'\n",
        "\n",
        "# Junção dos dados em um único dataframe:\n",
        "df_juncao = pd.concat([df_ba, df_er, df_ws], ignore_index=True)\n",
        "display(df_juncao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a33b9c1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalização dos dados:\n",
        "dados_normalizados = StandardScaler().fit_transform(df_juncao.loc[:, list(df_juncao.columns)[:-1]])\n",
        "\n",
        "# Cálculo do PCA com 2 componentes:\n",
        "pca = PCA(n_components=2)\n",
        "componentes_principais = pca.fit_transform(dados_normalizados)\n",
        "\n",
        "# Cálculo da variância explicada por componente principal:\n",
        "print('Variância por Componente Principal:', pca.explained_variance_ratio_)\n",
        "\n",
        "# Criação de DataFrame com o resultado:\n",
        "df_cps = pd.DataFrame(componentes_principais, columns=['CP1', 'CP2'])\n",
        "display(df_cps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7663e8e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostragem do resultado graficamente:\n",
        "tipos = ['BA', 'ER', 'WS']\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.xlabel('Primeira Componente Principal')\n",
        "plt.ylabel('Segunda Componente Principal')\n",
        "plt.title('Análise PCA de Medidas de Redes de Diferentes Topologias')\n",
        "for tipo in tipos:\n",
        "    indices_to_keep = df_juncao['tipo_rede'] == tipo\n",
        "    plt.scatter(df_cps.loc[indices_to_keep, 'CP1'], df_cps.loc[indices_to_keep, 'CP2'], ec='black', s=50)\n",
        "plt.legend(tipos, loc=(1.01, 0));\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
